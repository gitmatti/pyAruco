{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tutorial for using marker tracking with Nao"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This short tutorial will guide you through the most basic steps of using the C++ library ArUco (http://www.uco.es/investiga/grupos/ava/node/26) for marker tracking with NaoQi."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## ArUco\n",
      "\n",
      "ArUco is a \"minimal library for Augmented Reality applications based on OpenCv\". To get started, download aruco-1.2.4 from http://sourceforge.net/projects/aruco/files/?source=navbar and unpack it in a convenient location and install it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## PyAruco\n",
      "\n",
      "The code files setup.py and pyAruco.pyx provide a python wrapper for the most elementary functions from the c++ library. Place the files in /aruco-1.2.4/src/ and run:\n",
      "\n",
      "python setup.py build_ext --inplace\n",
      "\n",
      "If this successfully compiles you should end up with files pyAruco.cpp and pyAruco.so and you should be able to execute the following lines of code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the newly created c++ wrapper module\n",
      "import pyAruco\n",
      "import numpy as np\n",
      "import cv2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the module pyAruco only provides one class PyDetector which contains the minimal functionality \n",
      "# of the ArUco library to detect markers in images\n",
      "myDetector = pyAruco.PyDetector()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first you have to somehow specify the camera intrinsic parameters. This can be done in two ways:\n",
      "# reading from file\n",
      "myDetector.readCamParamFromXMLFile('./testdata/chessboard/intrinsics.yml')\n",
      "\n",
      "# by hand\n",
      "# first the camera matrix\n",
      "params_camMatrix= np.zeros((3,3));\n",
      "params_camMatrix[0,0]= 6.5286947951151615e+02 # the focal length\n",
      "params_camMatrix[1,0]=0\n",
      "params_camMatrix[2,0]=0\n",
      "params_camMatrix[0,1]=0\n",
      "params_camMatrix[1,1]= 6.5086903794514490e+02 # again the focal length\n",
      "params_camMatrix[2,1]=0\n",
      "params_camMatrix[0,2]=3.4759148384227956e+02 # these are the coordinates of\n",
      "params_camMatrix[1,2]=1.9930791284317061e+02 # the optical center in pixels\n",
      "params_camMatrix[2,2]=1\n",
      "# no distorsion for simplicity\n",
      "params_distortion= np.zeros((4,1))\n",
      "\n",
      "myDetector.setCamParam(params_camMatrix, params_distortion, np.array([640, 480]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's load a test image\n",
      "image = cv2.imread('./testdata/chessboard/chessboard.png',0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# finally, you can pass an image to the detector via the function PyDetector.detect()\n",
      "myDetector.detect(image)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's see which markers have been detected and what their parameters are\n",
      "\n",
      "# marker id\n",
      "ids = myDetector.getID()\n",
      "# marker coordinates in the image\n",
      "imagePosition = myDetector.getCenter()\n",
      "# an estimate of the 3d position of the marker \n",
      "# (a correct estimate requires the camera parameters to be correct, which is not the case right now...)\n",
      "absolutePosition = myDetector.getTvec()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naoqi and PyAruco\n",
      "\n",
      "The c++ wrapper can now also be used with naoqi (version 1.14). To this end, I wrote a naoqi-module NaoDetection in python available in PyNaoDetection.py. This module was originally thought to be locally available at naoqi startup, however this didn't work for me. That's why I always just ran python PyNaoDetection.py from my console. Also fine...\n",
      "In any case, if it works, the module should be available from python just like other naoqi-modules (ALMotion, ALMemory, AlRedBallTracking, etc...) and the following code should execute. Mind, however, that to date it has only been tested in Webots simulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from naoqi import ALProxy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IP = \"127.0.0.1\"\n",
      "PORT = 9559\n",
      "\n",
      "motionProxy = ALProxy(\"ALMotion\", IP, PORT)\n",
      "detectionProxy = ALProxy(\"NaoDetection\", IP, PORT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "RuntimeError",
       "evalue": "\tALNetwork::getModuleByName\n\tfailed to get module ALMotion http://127.0.0.1:9559",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-2b15d2fba201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPORT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9559\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmotionProxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALMotion\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdetectionProxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaoDetection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/mathias/Dokumente/Studium/Computational_Neuroscience/labrotation/hafner/pynaoqi-python-2.7-naoqi-1.14-linux64/naoqi.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0minaoqi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0minaoqi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/mathias/Dokumente/Studium/Computational_Neuroscience/labrotation/hafner/pynaoqi-python-2.7-naoqi-1.14-linux64/inaoqi.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inaoqi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mRuntimeError\u001b[0m: \tALNetwork::getModuleByName\n\tfailed to get module ALMotion http://127.0.0.1:9559"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# frist go to a position where the marker is visible\n",
      "motionProxy.setStiffnesses(['HeadPitch','HeadYaw'], 1.0)\n",
      "motionProxy.setAngles(['HeadPitch','HeadYaw'], [-0.25, 0.45], 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialization: specify joint names, sensor dimensions, camera width, camera height. Uses the bottom camera\n",
      "detectionProxy.init(['HeadPitch','HeadYaw','LShoulderRoll','LShoulderPitch','LElbowRoll','LElbowYaw'],\n",
      "                    ['imageX','imageY','spaceX','spaceY','spaceZ'],\n",
      "                    320, 240)\n",
      "# fire up the module in a separate thread\n",
      "detectionProxy.post.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "14"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = detectionProxy.getData()\n",
      "\n",
      "# is the marker detected?\n",
      "print 'Marker Detected:', data[0]\n",
      "# what is the corresponding joint position?\n",
      "print 'Joints:', data[1:7]\n",
      "# image coordinates\n",
      "print 'Image Position:', data[7:9]\n",
      "# absolute coordinates in frame TORSO\n",
      "print 'Absolute Position:', data[9:12]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Marker Detected: 1\n",
        "Joints: [-0.2500000298023224, 0.44999998807907104, 0.008700000122189522, 1.0689691265497459e-07, -0.03490658476948738, 2.1954882356567396e-08]\n",
        "Image Position: [0.05960521847009659, -0.14997877180576324]\n",
        "Absolute Position: [0.17527589201927185, 0.07735343277454376, 0.09738762676715851]\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}